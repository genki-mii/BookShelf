{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scoop.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhcZg/phWfoB2ZDiV4qaNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genki-mii/BookShelf/blob/master/scoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CDcRU3-d0WM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aefb2a4-cc73-4d37-ff40-04bf65274f28"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,365 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,781 kB]\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [442 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,690 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [252 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.3 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,208 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [865 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,130 kB]\n",
            "Fetched 11.1 MB in 3s (3,398 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 80.2 MB of archives.\n",
            "After this operation, 272 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 86.0.4240.198-0ubuntu0.18.04.1 [1,126 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 86.0.4240.198-0ubuntu0.18.04.1 [71.0 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 86.0.4240.198-0ubuntu0.18.04.1 [3,585 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 86.0.4240.198-0ubuntu0.18.04.1 [4,492 kB]\n",
            "Fetched 80.2 MB in 3s (24.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_86.0.4240.198-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_86.0.4240.198-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_86.0.4240.198-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_86.0.4240.198-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (86.0.4240.198-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrBplCQ1fBBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ff8f3b-86ad-4078-cfcf-435652cb0ff7"
      },
      "source": [
        "pip install requests-html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting requests-html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests-html) (2.23.0)\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests-html) (0.0.1)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/58/0b/85d15e21f660a8ea68b1e0286168938857391f4ec9f6d204d91c9e013826/pyquery-1.4.3-py3-none-any.whl\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/49/85f19d9ff908817b864deebf7f68211f9a6fc0b48746d372d970f60d01f5/parse-1.18.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (3.0.4)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests-html) (1.15.0)\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.9MB/s \n",
            "\u001b[?25hCollecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f3/90db6276dbc6cb1defa558251acc73c8e436ca8e1e2b38ec75786278de7c/pyee-7.0.4-py2.py3-none-any.whl\n",
            "Collecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/bc/857fff709f7ce9eabdc502d6fa71f4b7e964200b1bcd00f0a2f59667d1bf/tqdm-4.53.0-py2.py3-none-any.whl (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests-html) (4.6.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests-html) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: fake-useragent, parse\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=f7cfc4f6475680c072b5ecc49e5080a9d3dc4dcaef0612be008dda268ab403f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.18.0-cp36-none-any.whl size=24133 sha256=4f4434e11590a80674e5361600d61ab92863bd00a6125e5b49918411f4a7d080\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/53/09/869ca5781ede342254ffac09ca99461b008c3e5f8dd079b0c0\n",
            "Successfully built fake-useragent parse\n",
            "\u001b[31mERROR: pyppeteer 0.2.2 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: w3lib, fake-useragent, websockets, pyee, appdirs, tqdm, pyppeteer, cssselect, pyquery, parse, requests-html\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.18.0 pyee-7.0.4 pyppeteer-0.2.2 pyquery-1.4.3 requests-html-0.10.0 tqdm-4.53.0 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ja7OYeejKw"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "import urllib.request, urllib.error\n",
        "from requests_html import HTMLSession\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvcOc9Hdew_1"
      },
      "source": [
        "# ブラウザをheadlessモード（バックグラウンドで動くモード）で立ち上げてwebsiteを表示、生成されたhtmlを取得し、BeautifulSoupで綺麗にする。\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.implicitly_wait(10)\n",
        "driver.get(\"https://scooooooop.tv/ranking\")\n",
        "html = driver.page_source.encode('utf-8')\n",
        "soup = BeautifulSoup(html, \"html.parser\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl99svtstPc6"
      },
      "source": [
        "##scooptv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC4qLkB5f_A_"
      },
      "source": [
        "#日付のリストを作成する\n",
        "date1=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[0].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name1=[]\n",
        "hall=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[0].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall:\n",
        "  hall_name1.append(i.get_text())\n",
        "\n",
        "hall_name1 = [t.replace('\\n','') for t in hall_name1]\n",
        "hall_name1 = [t.replace(' ','') for t in hall_name1]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name1=[]\n",
        "event1=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[0].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event1:\n",
        "  event_name1.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df1=pd.DataFrame({\"店舗名\":hall_name1,\"イベント名\":event_name1})\n",
        "df1_1=df1.rename(index={0:date1})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date2=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[1].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name2=[]\n",
        "hall2=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[1].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall2:\n",
        "  hall_name2.append(i.get_text())\n",
        "\n",
        "hall_name2 = [t.replace('\\n','') for t in hall_name2]\n",
        "hall_name2 = [t.replace(' ','') for t in hall_name2]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name2=[]\n",
        "event2=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[1].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event2:\n",
        "  event_name2.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df2=pd.DataFrame({\"店舗名\":hall_name2,\"イベント名\":event_name2})\n",
        "df2_1=df2.rename(index={0:date2})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date3=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[2].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name3=[]\n",
        "hall3=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[2].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall3:\n",
        "  hall_name3.append(i.get_text())\n",
        "\n",
        "hall_name3 = [t.replace('\\n','') for t in hall_name3]\n",
        "hall_name3 = [t.replace(' ','') for t in hall_name3]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name3=[]\n",
        "event3=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[2].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event3:\n",
        "  event_name3.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df3=pd.DataFrame({\"店舗名\":hall_name3,\"イベント名\":event_name3})\n",
        "df3_1=df3.rename(index={0:date3})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date4=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[3].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name4=[]\n",
        "hall4=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[3].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall4:\n",
        "  hall_name4.append(i.get_text())\n",
        "\n",
        "hall_name4 = [t.replace('\\n','') for t in hall_name4]\n",
        "hall_name4 = [t.replace(' ','') for t in hall_name4]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name4=[]\n",
        "event4=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[3].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event4:\n",
        "  event_name4.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df4=pd.DataFrame({\"店舗名\":hall_name4,\"イベント名\":event_name4})\n",
        "df4_1=df4.rename(index={0:date3})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date5=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[4].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name5=[]\n",
        "hall5=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[4].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall5:\n",
        "  hall_name5.append(i.get_text())\n",
        "\n",
        "hall_name5 = [t.replace('\\n','') for t in hall_name5]\n",
        "hall_name5 = [t.replace(' ','') for t in hall_name5]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name5=[]\n",
        "event5=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[4].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event5:\n",
        "  event_name5.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df5=pd.DataFrame({\"店舗名\":hall_name5,\"イベント名\":event_name5})\n",
        "df5_1=df5.rename(index={0:date3})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date6=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[5].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name6=[]\n",
        "hall6=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[5].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall6:\n",
        "  hall_name6.append(i.get_text())\n",
        "\n",
        "hall_name6 = [t.replace('\\n','') for t in hall_name6]\n",
        "hall_name6 = [t.replace(' ','') for t in hall_name6]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name6=[]\n",
        "event6=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[5].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event6:\n",
        "  event_name6.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df6=pd.DataFrame({\"店舗名\":hall_name6,\"イベント名\":event_name6})\n",
        "df6_1=df6.rename(index={0:date6})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date7=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[6].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name7=[]\n",
        "hall7=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[6].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall7:\n",
        "  hall_name7.append(i.get_text())\n",
        "\n",
        "hall_name7 = [t.replace('\\n','') for t in hall_name7]\n",
        "hall_name7= [t.replace(' ','') for t in hall_name7]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name7=[]\n",
        "event7=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[6].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event7:\n",
        "  event_name7.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df7=pd.DataFrame({\"店舗名\":hall_name7,\"イベント名\":event_name7})\n",
        "df7_1=df7.rename(index={0:date7})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date8=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[7].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name8=[]\n",
        "hall8=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[7].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall8:\n",
        "  hall_name8.append(i.get_text())\n",
        "\n",
        "hall_name8 = [t.replace('\\n','') for t in hall_name8]\n",
        "hall_name8 = [t.replace(' ','') for t in hall_name8]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name8=[]\n",
        "event8=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[7].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event8:\n",
        "  event_name8.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df8=pd.DataFrame({\"店舗名\":hall_name8,\"イベント名\":event_name8})\n",
        "df8_1=df8.rename(index={0:date8})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date9=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[8].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name9=[]\n",
        "hall9=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[8].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall9:\n",
        "  hall_name9.append(i.get_text())\n",
        "\n",
        "hall_name9 = [t.replace('\\n','') for t in hall_name9]\n",
        "hall_name9 = [t.replace(' ','') for t in hall_name9]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name9=[]\n",
        "event9=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[8].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event9:\n",
        "  event_name9.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df9=pd.DataFrame({\"店舗名\":hall_name9,\"イベント名\":event_name9})\n",
        "df9_1=df9.rename(index={0:date9})\n",
        "\n",
        "#日付のリストを作成する\n",
        "date10=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[9].find(\"p\").text\n",
        "\n",
        "#ホール名のリストを作成する\n",
        "hall_name10=[]\n",
        "hall10=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[9].find_all(\"li\",attrs={\"class\",\"schedule-hall\"})\n",
        "for i in hall10:\n",
        "  hall_name10.append(i.get_text())\n",
        "\n",
        "hall_name10 = [t.replace('\\n','') for t in hall_name10]\n",
        "hall_name10 = [t.replace(' ','') for t in hall_name10]\n",
        "\n",
        "#イベント名のリストを作成する\n",
        "event_name10=[]\n",
        "event10=soup.find_all(\"div\",attrs={\"class\":\"date-rows clearfix\"})[9].find_all(\"ul\",attrs={\"class\",\"schedule-title\"})\n",
        "for k in event10:\n",
        "  event_name10.append(k.get_text())\n",
        "\n",
        "#各リストをデータフレームに変換\n",
        "df10=pd.DataFrame({\"店舗名\":hall_name10,\"イベント名\":event_name10})\n",
        "df10_1=df6.rename(index={0:date10})\n",
        "\n",
        "df_all=pd.concat([df1_1,df2_1,df3_1,df4_1,df5_1,df6_1,df7_1,df8_1,df9_1,df10_1])\n",
        "df_all.to_csv(\"scoop.csv\", encoding=\"UTF-8\",index=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiy-R3bO9jS5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}